{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import MultinomialHMM\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialHMM(n_components=2)\n",
    "model.startprob_ = np.array([0.5, 0.5])\n",
    "model.transmat_ = np.array([[11/12, 1/12],\n",
    "                            [19/20, 1/20]])\n",
    "model.emissionprob_ = np.array([[1/6, 1/6, 1/6, 1/6, 1/6, 1/6],\n",
    "                                [0.5, 0.1, 0.1, 0.1, 0.1, 0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.19359\n",
      "1: 0.16213\n",
      "2: 0.16031\n",
      "3: 0.16192\n",
      "4: 0.16149\n",
      "5: 0.16056\n"
     ]
    }
   ],
   "source": [
    "results = model.sample(100000)\n",
    "c = Counter(results[0].flatten())\n",
    "for i in range(len(c)):\n",
    "    print(str(i) + ': ' + str(c[i] / 100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I approached the first problem by drawing a FSM with two states. This drawing is provided in the pdf along with the transition probabilities and emission probabilities. After modeling this using HMM's Multinomial class, I was able to sample this distribution 100000 times. We see that the probability that the emitted probability is a 1 is approximately 0.193, which is slightly higher than 1/6. The other values had probability slightly below 1/6, which is attributed to the lower probability on the unfair die."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: 0.90999997\n",
      "100: 0.919\n",
      "1000: 0.92059994\n",
      "10000: 0.92021\n"
     ]
    }
   ],
   "source": [
    "num_list = [10, 100, 1000, 10000]\n",
    "\n",
    "for num_symbols in num_list:\n",
    "    samples = np.empty((10, num_symbols), dtype=np.int8)\n",
    "    true_seq = np.empty((10, num_symbols), dtype=np.int8)\n",
    "    for i in range(10):\n",
    "        result = model.sample(num_symbols)\n",
    "        samples[i,:], true_seq[i,:] = (result[0].ravel(), result[1].ravel())\n",
    "\n",
    "    accuracies = np.empty(10, dtype=np.float32)\n",
    "\n",
    "    for i in range(10):\n",
    "        predict_seq = model.decode(samples[i,:].reshape(-1,1))[1].ravel()\n",
    "        accuracies[i] = np.equal(predict_seq, true_seq[i,:]).astype(int).sum() / num_symbols\n",
    "        \n",
    "    print(str(num_symbols) + ': ' + str(accuracies.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, I simulated draws of 10, 100, 1000, 10000, each 10 times and used MulinomialHMM.decode to predict what the original sequence states were given the sample that I drew. Once I had the predicted states and the true states, I could just count in how many places they varied. I recorded this for each of the 10 epochs and then averaged the accuracies. It looks like sampling a larger number of points did not help accuracy, as it was almost a constant ~0.915 throughout all epochs of the simulation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
